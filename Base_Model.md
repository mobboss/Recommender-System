# 3. 基础模型
在搜索，推荐，广告中涉及到模型的优化，通常会运用NLP，CV中的一些方法，同时，NLP以及CV中的一些方法在内容结构化方面有着重要的作用。
## 3.1. NLP
- [2014]. [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)
  - 简介：CNN模型解决文本分类的问题
  - 阅读笔记：[CNN在文本建模中的应用TextCNN](http://felixzhao.cn/Articles/article/12)
- [2016]. [Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf)
  - 简介：文本分类工具FastText的文章，FastText在当前的文本分类任务中依旧是很好的工具
  - 阅读笔记：[文本分类fastText算法解析](http://felixzhao.cn/Articles/article/13)
- [2017]. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
  - 简介：Attention的经典文章，对Transformer的原理做了详细介绍
  - 阅读笔记：[Transformer的基本原理](http://felixzhao.cn/Articles/article/36)
- [2018]. [Deep Contextualized Word Representations](https://www.researchgate.net/publication/323217640_Deep_contextualized_word_representations)
  - 简介：预训练模型ELMo
  - 阅读笔记：[Embeddings from Language Models（ELMo）](http://felixzhao.cn/Articles/article/29)
- [2018]. [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
  - 简介：预训练模型GPT
  - 阅读笔记：[GPT：Generative Pre-Training](http://120.53.236.240/Articles/article/33)
- [2018]. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
  - 简介：预训练模型BERT
  - 阅读笔记：[BERT模型解析](http://felixzhao.cn/Articles/article/38)

## 3.2. CV
- [2020]. [An image is worth 16x16 words: Transformers for image recognition at scale](https://arxiv.org/abs/2010.11929)
  - 简介：提出了基于Transformer中Encoder的图像分类模型ViT（Vision Transformer）
  - 阅读笔记：[Vision Transformer（ViT）](http://felixzhao.cn/Articles/article/56)